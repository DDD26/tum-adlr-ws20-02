{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndimage\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "from typing import Callable\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO, A2C, SAC\n",
    "from stable_baselines3.common.cmd_util import make_vec_env\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from motion_env import MPEnv\n",
    "from RL_env import RLEnv,CustomActorCriticPolicy\n",
    "from generate_env import generate_env, make_env, ExpertDataSet\n",
    "from pretrain import pretrain_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.array([[10,10],[40,50],[15,35]])\n",
    "size = np.array([[20,20],[10,30],[20,30]])\n",
    "bound = np.array([64,64])\n",
    "start = np.array([0.1,0.1])\n",
    "end = np.array([6.3,6.3])\n",
    "opt_num = 10\n",
    "sp_num = 5\n",
    "co_num = 20\n",
    "w = 0.1\n",
    "mpenv = MPEnv(pos,size,bound,start,end)\n",
    "env = RLEnv(mpenv,2)\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train_num = 100\n",
    "opt_num = 10\n",
    "sup_dim = 40\n",
    "ob_num = 10\n",
    "limit = np.array([10,20])\n",
    "step = 2\n",
    "\n",
    "### generate easy train environments ###\n",
    "env_train_easy_list1 = []\n",
    "for i in range(env_train_num):\n",
    "    env_train_easy_list1.append(generate_env(ob_num,limit,opt_num,step,sup_dim = sup_dim))\n",
    "env_train_easy_list = [make_env(env,i) for i,env in enumerate(env_train_easy_list1)] \n",
    "env_train_easy = DummyVecEnv(env_train_easy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate supervision data ###\n",
    "env_train = env_train_easy\n",
    "mode = 'generate'\n",
    "if mode == 'generate':\n",
    "    exp_data = env_train.env_method('supervision')\n",
    "    exp_obs = [i[0] for i in exp_data]\n",
    "    exp_act = [i[1] for i in exp_data]\n",
    "    exp_obs = np.concatenate(np.stack(exp_obs)).astype(np.uint8)\n",
    "    exp_act = np.concatenate(np.stack(exp_act)).astype(np.float32)\n",
    "    np.save('obs1e6',exp_obs)\n",
    "    np.save('act1e6',exp_act)\n",
    "if mode == 'load':\n",
    "    exp_obs = np.load('obs1e6.npy')\n",
    "    exp_act = np.load('act1e6.npy')\n",
    "exp_data = ExpertDataSet(exp_obs, exp_act)\n",
    "train_size = int(0.95 * len(exp_data))\n",
    "test_size = len(exp_data) - train_size\n",
    "exp_train, exp_test = random_split(exp_data, [train_size, test_size])\n",
    "### generate supervision data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = PPO(CustomActorCriticPolicy, env_train_easy, n_steps=40, gamma=1, verbose=1)\n",
    "pretrain_agent(student,\n",
    "               exp_train,\n",
    "               exp_test,\n",
    "               batch_size=64,\n",
    "               epochs=10,\n",
    "               scheduler_gamma=0.7,\n",
    "               learning_rate=1.0,\n",
    "               log_interval=100,\n",
    "               no_cuda=False,\n",
    "               seed=1,\n",
    "               test_batch_size=16)\n",
    "student.learn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test_sample = generate_env(10,np.array([10,20]),10,2)\n",
    "#env_test_sample  = env_train_easy_list1[12]\n",
    "obs = env_test_sample.reset()\n",
    "x = env_test_sample.pos\n",
    "plt.imshow(env_test_sample.MPEnv.dis)\n",
    "for i in range(200):\n",
    "    x = x - 0.1*env_test_sample.MPEnv.ob_der_fun(x)\n",
    "x = env_test_sample.MPEnv.all_points(x,0)\n",
    "x = env_test_sample.MPEnv.real2pix(x)\n",
    "plt.plot(x.T[1],x.T[0],'v-w')\n",
    "\n",
    "for step in range(200):\n",
    "    action, _ = student.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env_test_sample.step(action)\n",
    "    #if done:\n",
    "    #    print(\"Goal reached!\", \"reward=\", reward,\"step=\",step)\n",
    "    #    break\n",
    "x1 = env_test_sample.pos\n",
    "x1 = env_test_sample.MPEnv.all_points(x1,0)\n",
    "x1 = env_test_sample.MPEnv.real2pix(x1)\n",
    "plt.plot(x1.T[1],x1.T[0],'o-r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# successful rate evaluation\n",
    "env_test_easy_num = 1000\n",
    "env_test_hard_num = 100\n",
    "\n",
    "# generate easy test environments\n",
    "env_test_easy_list1 = []\n",
    "for i in range(env_test_easy_num):\n",
    "    env_test_easy_list1.append(generate_env(10,np.array([10,20]),10,2))\n",
    "# generate hard test environments\n",
    "env_test_hard_list1 = []\n",
    "count = 0\n",
    "while(count<env_test_hard_num):\n",
    "    env_try = generate_env(10,np.array([10,20]),10,2)\n",
    "    x0 = env_try.MPEnv.initial()\n",
    "    for j in range(200):\n",
    "        x0 = x0 - 0.1*env_try.MPEnv.ob_der_fun(x0)\n",
    "    if not env_try.MPEnv.collision(x0):\n",
    "        env_test_hard_list1.append(env_try)\n",
    "        count += 1\n",
    "        print(count,'hard cases for test found')\n",
    "    env_try.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "### easy test benchmark ###\n",
    "n_steps = 200\n",
    "lr = 0.1\n",
    "result_easy = np.zeros((4,))\n",
    "i = 0\n",
    "for env_test in env_test_easy_list1:\n",
    "    obs = env_test.reset()\n",
    "    x0 = env_test.pos\n",
    "    for step in range(n_steps):\n",
    "        x0 = x0 - lr*env_test.MPEnv.ob_der_fun(x0)\n",
    "    for step in range(n_steps):\n",
    "        action, _ = student.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env_test.step(action)\n",
    "        if done:\n",
    "            #print(\"Goal reached!\", \"reward=\", reward,\"step=\",step)\n",
    "            break\n",
    "    if done and env_test.MPEnv.collision(x0):\n",
    "        result_easy[0] += 1\n",
    "    if not done and not env_test.MPEnv.collision(x0):\n",
    "        result_easy[1] += 1\n",
    "    if done and not env_test.MPEnv.collision(x0):\n",
    "        result_easy[2] += 1\n",
    "    if not done and env_test.MPEnv.collision(x0):\n",
    "        result_easy[3] += 1\n",
    "    env_test.close()\n",
    "    if (i+1) % 50 == 0:\n",
    "        print((i+1)/len(env_test_easy_list1),'complete')\n",
    "    i += 1\n",
    "result_easy /= len(env_test_easy_list1)\n",
    "print(\"result_list_easy:\", result_easy)\n",
    "rl_success = result_easy[0]+result_easy[2]\n",
    "gd_success = result_easy[0]+result_easy[3]\n",
    "print('success_rl: %.2f%%'  % (rl_success*100))\n",
    "print('success_gd: %.2f%%'  % (gd_success*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "### hard test benchmark ###\n",
    "n_steps = 200\n",
    "lr = 0.1\n",
    "result_hard_GD = 0\n",
    "result_hard_RL = 0\n",
    "for env_test in env_test_hard_list1:\n",
    "    obs = env_test.reset()\n",
    "    x0 = env_test.pos\n",
    "    for step in range(n_steps):\n",
    "        action, _ = student.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env_test.step(action)\n",
    "        if done:\n",
    "            result_hard_RL += 1\n",
    "            break\n",
    "    env_test.close()\n",
    "result_hard_RL /= len(env_test_hard_list1)\n",
    "print(\"result_list_hard_RL\", result_hard_RL)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
